{
  "help": {
    "default": "Lijst van mogelijke commando's zijn\n\n- help\n- nlp\n- uva\n- voorbeelden\n- visie\n- toekomst\n- kader\n- clear\n\nTyp help --<commando> voor meer informatie.",
    "flags": {
      "help": "Geeft een lijst van alle mogelijke commando's. Typ \"help\" voor meer informatie",
      "nlp": "Geeft meer informatie over NLP. Typ \"nlp\" voor meer informatie.",
      "uva": "Geeft meer informatie over NLP onderzoek aan de UvA. Typ \"uva\" voor meer informatie.",
      "voorbeelden": "Geeft voorbeelden van onderzoeken binnen NLP. Typ \"voorbeelden\" voor meer informatie.",
      "visie": "Geeft meer informatie over onze visie aangaande onderzoek binnen de informatica. Typ \"visie\" voor meer informatie.",
      "toekomst": "Geeft onze kijk op NLP in de toekomst. Typ \"toekomst\" voor meer informatie.",
      "kader": "Geeft meer informatie over hoe NLP in het kader van de informatica past. Typ \"kader\" voor meer informatie.",
      "clear": "Maakt het scherm leeg."
    }
  },
  "nlp": {
    "default": "lijst van mogelijke opties is\n\n--ki\n--interactie\n\nTyp nlp --<optie> voor meer informatie.",
    "flags": {
      "ki": "Hier komt iets te staan over KI",
      "interactie": "Hier komt iets te staan over interactie"
    }
  },
  "uva": {
    "default": "lijst van mogelijke opties is\n\n--onderzoek1\n--onderzoek2\n--onderzoeksgroepen\n\nTyp uva --<optie> voor meer informatie.",
    "flags": {
      "onderzoek1": "\nAan de UvA wordt veel onderzoek gedaan naar Natural Language Processing. Zo hebben Maarten de Rijke (ILPS) en Evangelos Kanoulas (ILPS) een onderzoek gedaan in samenwerking met Christophe Van Gysel van Apple dat werd gesubsidieerd door Bloomberg en Google. Dit onderzoek heet Mix ’n Match: Integrating Text Matching and Product Substitutability within Product Search.\n\nDit onderzoek combineert productomschrijving met substitueerbaarheid (producten zijn substitueerbaar als ze aan dezelfde behoefte van de consument kunnen voorzien) om de relevantie van product zoekresultaten te verbeteren. Dit zorgt ervoor dat zoekresultaten relevanter zijn maar wel minder divers. En is een oplossing voor het feit dat producten met een andere of slechtere productomschrijving vaak niet aan de zoek query voldoen en dus niet getoond worden. Het experiment is gedaan aan de hand van drie onderzoeksvragen:\n\nOm vraag 1 te beantwoorden is er een vergelijking gemaakt tussen intrinsieke en extrinsieke methodes om product overeenkomstigheid te verwerken in het latent vector model, met gebruik van lexicografische en semantische zoekresultaat modellen.\nOm vraag 2 te beantwoorden is er een parameter gevoeligheidsanalyse uitgevoerd waar het effect van de interpolatie hyperparameter alpha wordt gemeten.\nOm vraag 3 te beantwoorden is er gebruik gemaakt van de product zoekresultaten benchmarks van Amazon. Waarbij de top 100 producten van een query worden geranked door een latent vector model die gebruik maakt van a) alleen tekst en b) tekst en product vergelijkbaarheid. De verschillen in queries bij beiden wordt onderzocht.\n\nDe conclusie is dat het toevoegen aan van substitueerbaarheid inderdaad betere zoekresultaten oplevert bij producten maar wellicht niet bij het zoeken naar documenten en teksten.\n\nDit onderzoek is zeer relevant nagaande dat Amazon alleen al 250 miljoen verschillende producten te koop heeft. [1] De online verkoop wordt alleen maar groter en het aantal producten dat online te koop is wordt ook groter. Hierdoor wordt het steeds belangrijker dat een query de juiste resultaten oplevert. Dit is zowel voor de consument als de retailer erg belangrijk.\n\nDit onderzoek is uitgevoerd door het Information and Language Processing Systems instituut van de UvA dat zich veel bezig houdt met het verbeteren van zoekresultaat relevantie. In de informatica staat dit dan ook in het kader “toepassingen” waarbij informatie technologie toegepast wordt in alledaags gebruik.\n\nurl: https://www.scrapehero.com/how-many-products-does-amazon-sell-worldwide-october-2017/\n",
      "onderzoek2": "\nEen toepassing van Natural Language Processing is het automatisch kunnen samenvatten van teksten. Er zijn binnen dit onderzoeksgebied drie verschillende methoden van automatisch samenvatten:\n\n1. Op basis van extractie, waarbij zinnen of zinsdelen uit een tekst worden geselecteerd en gebruikt in de samenvatting zonder deze aan te passen.\n2. Op basis van abstractie, waarbij zinnen of zinsdelen geparafraseerd worden om de informatiedichtheid van de samenvatting op die manier zo hoog mogelijk te krijgen. Hierbij is dus niet alleen Natural Language Processing nodig, maar ook Natural Language Generation.\n3. Met behulp van menselijke tussenkomst, waarbij de computer zelf geen volledige samenvatting schrijft, maar mogelijke zinnen selecteert die vervolgens door een mens worden gefilterd.\n\nDe UvA heeft vorig jaar, in samenwerking met onder andere Shandong University en Windows onderzoek naar gedaan naar samenvatten op basis van extractie met behulp van Deep Neural Networks. Dit onderzoek is uitgevoerd binnen de Information and Language Processing Systems onderzoeksgroep.\n\nIn het onderzoek wordt een Deep Neural Network-model, SRSum, geïntroduceerd dat door middel van Natural Language Processing automatisch teksten kan samenvatten. Het samenvatten van teksten kan ingewikkeld zijn, omdat het zowel belangrijk is om individuele zinnen te begrijpen als de context waarin deze zinnen staan. Daarnaast moet de zin te maken hebben met onderwerp van de gehele tekst, vaak beschreven in de titel. Tot slot zijn er vaak een aantal termen die terug moeten komen in een samenvatting. Het ontworpen model houdt rekening met al deze zaken, aan de hand van vijf sub-modellen:\n\n1. PriorSum bepaalt de inhoud van een zin: welke concepten worden besproken?\n2. SFSum evalueert oppervlakkige informatie zoals lengte en positie van een zin.\n3. CSRSum bepaalt voor elke zin in hoeverre deze de tekst in de buurt samenvat.\n4. TSRSum evalueert in hoeverre een zin correspondeert met de titel van een tekst, op basis van semantiek.\n5. QSRSum evalueert de relevantie van elke zin op basis van gegeven termen.\n\nOp basis van van deze vijf sub-modellen krijgt elke zin in de tekst een score. Op basis van deze score wordt met behulp van het zogenaamde Greedy-algoritme bepaald of de zin wel of niet moet voorkomen in de samenvatting. In vergelijking met andere modellen presteert SRSum het beste, maar er zitten ook nadelen aan het model. Omdat het ontwerp van het netwerk zo complex is, kost het trainen van het netwerk veel tijd.\n\nHet automatisch samenvatten van teksten klinkt allereerst natuurlijk aantrekkelijk voor bijvoorbeeld het samenvatten van studieboeken, maar er zijn veel meer toepassingen. Zo wordt het ook steeds meer gebruikt in zoekmachines. Door korte samenvattingen te genereren van webpagina's weet de gebruiker eerder of de betreffende pagina antwoord geeft op de vraag dan door bijvoorbeeld alleen de eerste alinea te tonen.\n",
      "onderzoeksgroepen": "\nDe UvA heeft twee onderzoeksgroepen die zich bezighouden met onderzoek naar Natural Language Processing. Dit zijn ILPS (information and language processing systemsn) en ILLC (Institute for Logic, Language and Computation). Beide instituten zijn deel van het Instituut van Informatica aan de Universiteit van Amsterdam.\n\nILPS: Het onderzoek van het ILPS is vooral gericht op het intelligent verwerken en verkrijgen van (grote hoeveelheden van) informatie. Er wordt gewerkt aan information retrieval, machine translation en language technologie. Maar ook aan het analyseren van structurele informatie uit sociale netwerken of gelinkte data en tot slot aan het analyseren van user behavior. Het ILPS is onderverdeeld in vier research labs:\n\n- AIRLAB (AI for retail)\n- Human Machine Intelligence Lab\n- Information Retrieval Lab\n- Language Technology Lab.\n\nOok heb ik vernomen dat ze bezig zijn met het opzetten van het “Police Lab” waar information retrieval systemen voor de politie worden onderzocht (dit is zeker relevant met de nieuwe Wiv). Het ILPS doet veel onderzoek met Natural Language Processing. En werkt nauw samen met andere onderzoeksgroepen zowel in de academia als daarbuiten. Het ILPS wordt gesubsidieerd door onder andere het NWO (Nederlandse organisatie Wetenschappelijk Onderzoek), het KNAW (Koninklijke Nederlandse Akademie van Wetenschappen), de EU en particuliere organisaties.\n\nILLC: Het ILLC gaat dieper in op de wiskundige eigenschappen van informatie. Hoe kunnen we bijvoorbeeld de informatiestromen tussen mens en computer omschrijven? De missie van het ILLC is om formele eigenschappen van informatie te onderzoeken. Dat will zeggen logische structuur, algoritmische eigenschappen van het coderen, verzenden en begrijpen van informatie. Het doel van het onderzoek is het verkrijgen en ontwikkelen van logische systemen  die de grote hoeveelheid van verschillende soorten inforamatie (taal, afbeeldingen, muziek) kan verwerken. Het ILLC werkt nauw samen met internationale research groepen onder andere in Parijs, Londen en India. Het ILLC is een bredere onderzoeksgroep. Natural Language Processing is een onderdeel wat voor alle brede aspecten wordt gebruikt.\n"
    }
  },
  "voorbeelden": {
    "default": "lijst van mogelijke opties is\n\n--spraaktekst\n--vertalen\n--zoekmachines\n\nTyp voorbeelden --<optie> voor meer informatie.",
    "flags": {
      "spraaktekst": "\nBij het herkennen van spraak wordt snel gedacht aan speech-to-text technologieën. Hoewel dit een techniek is die gebruikt wordt bij Natural Language Processing, is dit niet hetzelfde als NLP. Het verwerken van spraakopdrachten met NLP kan misschien het beste geïllustreerd aan de hand van een voorbeeld. In de afgelopen jaren zijn de 'smart speakers' zoals de Amazon Echo of Google Home steeds populairder geworden. Deze speakers hebben een ingebouwde virtuele assistent die spraakbestuurde commando's en vragen kan uitvoeren en beantwoorden. Om de door jou gestelde vraag om te zetten in voor de computer begrijpelijke data wordt speech-to-text gebruikt, ook wel 'Automated Speech Recognition', of ASR genoemd. Hoewel deze data nu begrepen kan worden door de computer, is het nog niet bekend wat er moet gebeuren met die data. Hiervoor is NLP nodig. Met behulp van Natural Language Processingmodellen wordt bepaald wát er precies gezegd is, en dit wordt gekoppeld aan een door de computer uit te voeren commando.\n\nTekstherkenning werkt eigenlijk precies hetzelfde, behalve dat de gestelde vragen of commando's al getypt zijn en dus direct verwerkt kunnen worden door een Natural Language Processingmodel. Deze toepassing kom je veel tegen in chatbots, bijvoorbeeld bij digitale helpdesks. Door hiervoor NLP te gebruiken in plaats van alleen simpele commando's kunnen klanten op een makkelijkere manier vragen stellen en kan er gerichter antwoord worden gegeven.\n",
      "vertalen": "\nMachine Translation is de techniek waarbij de ene natuurlijke taal door de computer vertaald wordt in een andere natuurlijke taal. Het meest bekende voorbeeld hierbij is Google Translate. Deze service kan niet alleen op basis van de context van een gegeven tekst de juiste vertaling geven, maar kan zelfs de taal waarin een tekst geschreven is herkennen, allemaal dankzij NLP. Er zijn verschillende aanpakken voor Machine Translation. De meest simpele manier is om voor elk gegeven woord in een tekst de letterlijke woordenboek- vertaling te geven, maar dan krijg je al snel rare vertalingen. Betere technieken zijn vertalingen op basis van statistiek of bekende voorbeelden, vaak door middel van machine learning. Hierbij wordt de context waarbinnen een woord gebruikt wordt ook meegenomen, en is de kans op een kloppende, goedlopende vertaling dus groter.\n",
      "zoekmachines": "\nToen zoekmachines nog relatief nieuw waren was het gebruikelijk om een zoekopdracht uit enkel een aantal korte termen te laten bestaan, zoals 'restaurant amsterdam'. Meer termen, en de zoekresulaten werden al snel een stuk minder nauwkeurig. Geavanceerde zoekfuncties, zoals het gebruik van booleaanse operatoren was mogelijk, maar werden erg weinig gebruikt, omdat veel gebruikers niet wisten hoe ze deze functies moesten gebruiken. Met de komst van Natural Lanuage Processing, en specifiek zogenoemde Semantic Search is het mogelijk om meer conversationele vragen aan de zoekmachine te stellen, zoals 'welke restaurants in amsterdam zijn nu open?'. Met behulp van Semantic Search wordt er gekeken naar de betekenis van de gestelde vraag, zodat bepaald kan worden waar de gebruiker precies naar op zoek is. Op die manier kunnen de beste zoekresultaten worden gevonden. Semantic Search kan naast algemene zoekmachines als Google ook worden toegepast op domeinspecifieke zoekmachines. Een goed voorbeeld hiervan is Semantic Scholar, een zoekmachine speciaal voor wetenschappelijke artikelen. Op basis van de titel van een artikel kan met behulp van Semantic Search worden bepaald welke artikelen gerelateerd zijn aan het gegeven artikel.\n"
    }
  },
  "visie": {
    "default": "lijst van mogelijke opties is\n\n--jorik\n--florine\n--emil\n--renee\n\nTyp visie --<optie> voor meer informatie.",
    "flags": {
      "emil": "\nHoewel informatica al een lange tijd bestond toen ik ter wereld kwam, is het nog steeds een relatief jonge wetenschap. Dat houdt in dat er nog steeds zo veel valt te onderzoeken, wat voor ons aspirerende informatici een unieke kans is. Wij kunnen aan het begin staan van iets totaal nieuws, met als voorbeeld NLP. Als iemand 30 jaar geleden had gezegd dat wij later met een computer kunnen praten, zouden we die persoon voor gek verklaren. Door de ontwikkelingen in de laatste 10 jaar is dit echter een realiteit geworden. Ik zie wetenschap daarom als een creatieve en experimentele bezigheid. Mensen moeten durven buiten het standaard patroon te denken, en dat hoeft daarom niet altijd op basis van dingen die we al weten. Kijk bijvoorbeeld naar een persoon als Elon Musk, die een kolonie wil starten op Mars. Sommige mensen verklaren hem ook voor gek, maar dit is naar mijn mening wel een persoon die ons tot het uiterste drijft op het gebied van technologische ontwikkelingen. Ook denk ik dat iedereen wetenschappelijk bezig kan zijn, en dat het niet alleen maar binnen een universiteit of andere instanties hoeft te gebeuren. Als iemand thuis iets programmeert of maakt wat wij nog niet hebben gezien, kan dat ook worden beschouwd als wetenschap. Ik denk daarom dat wetenschap voor iedereen is, en vooral binnen de informatica. De rekenkracht waarover we nu beschikken en de betaalbaarheid van computers op dit moment maakt het mogelijk om iedereen creatief te laten zijn. Dit zou iedereen moeten in zijn voordeel moeten gebruiken.\n",
      "jorik": "\nIk geloof dat natural language processing de wereld voorgoed verandert in sociale zin. Door natural language processing zullen mensen in de toekomst geen last taal barriers ondervinden. Bovendien zullen makkelijkere talen kunnen ontstaan door het begrip van taal en het begrip van hoe een taal begrepen wordt. Door meer onderzoek naar language processing, kunnen ze uiteindelijk in real-time language processen. Computers zullen in real time de essentie kunnen overbrengen van een speech, conversatie of bericht.\n\nUiteraard verandert de wereld ook in linguïstische zin, waarbij automaten afgaan op wat gezegd wordt, in plaats van de knopjes die worden ingetoetst. Gebruikersvriendelijkheid zal gericht zijn op de communicatie tussen mens en machine, in plaats van consument en verkoper. Beveiliging zou ook sterk kunnen toenemen door middel van spraakherkenning. Hoewel ik in twijfel trek of dit de dominante beveiligingsmethode wordt.\n\nDoor op linguïstisch en sociologisch front zulke stappen te maken, kan er op meerdere fronten gebruik van gemaakt worden. Wetenschappers zouden bijvoorbeeld speeches in hun moedertaal kunnen geven, wat dan vertaalt wordt voor elk persoon in het publiek individueel. Als samenvattingen van artikels automatisch gemaakt kunnen worden, dat zal vooral voor het nieuws en andere vormen van informatieverstrekking toegevoegde waarde opleveren.\n",
      "florine": "\nHet mooie aan wetenschap vind ik dat je niet bang moet zijn voor het feit dat je iets niet weet. Daar is onderzoek immers voor! Men is soms van mening dat onderzoek alleen waardevol is als het iets bijdraagt aan de samenleving. Ik ben het daar niet mee eens, om twee redenen. Ten eerste: wie bepaalt wanneer een onderzoek of ontdekking bijdraagt aan de samenleving, en waar wordt de grens getrokken tussen het wel- en niet-waardevol zijn van een bepaald onderzoek? Daarnaast heeft de geschiedenis bewezen dat er talloze onderzoeken zijn gedaan in eerste instantie niet direct iets hebben bijgedragen aan de samenleving, maar waarvan ze jaren later een inspiratie of katalysator zijn geweest voor iets dat wel heeft bijgedragen aan de samenleving. Hoewel ik dus vind dat onderzoek gedaan moet kunnen worden zonder dat dit direct iets moet kunnen opleveren voor de wereld om ons heen, betekent dit naar mijn mening niet dat er niet moeten worden nagedacht over de (mogelijk negatieve) consequenties van ontdekkingen. Zeker met het oog op informatica vind ik dat er van te voren goed moet worden nagedacht over de eventuele gevolgen van bijvoorbeeld een nieuwe technologie. Neem het voorbeeld van gegevensverzameling. Het wordt steeds makkelijker om snel en op grote schaal data van mensen te verzamelen en verwerken, en hierbij draagt NLP ook aan bij. Ik vind dat er goed moet worden nagedacht in hoeverre we alles van elkaar willen weten, en in hoeverre het ook nuttig is om alles van elkaar te weten.\n",
      "renee": "\nIk geloof niet in één enkele waarheid. De waarheid is gebaseerd op ieders perceptie en perceptie verschilt van persoon tot persoon. Er is dus wat mij betreft geen universele waarheid. Ook niet in de wetenschap. Juist in de wetenschap kan je talloze onderzoeken vinden die iets ontkrachten terwijl je ook talloze onderzoeken kan vinden over dat zelfde onderwerp die het juist bewijzen. Ditzelfde geldt voor deductivisme. Deductivisme houdt theorieën aan als meest belangrijk, terwijl inductivisme observatie als meest belangrijk ziet. Inductie houdt in dat wat wij als waarheid beschouwen in een aantal gevallen, ook geldt in alle andere gevallen. Ik ben het hier mee oneens. Kijk bijvoorbeeld naar de witte zwaan. Totdat de zwarte zwaan voor het eerst gezien werd was de algemene waarheid dat zwanen wit zijn. Dit bleek later niet waar te zijn. Uiteindelijk gaat het wat mij betreft om een combinatie van de twee. Als er geen bijzondere theorieën bedacht worden, is er ook niks om op te experimenteren. Zeker op het gebied van de Informatica is er nog veel te ontdekken door middel van wetenschappelijk onderzoek. Wat wij wellicht nu als waar beschouwen, kan in de toekomst ontkracht worden. En wat wij nu juist als onmogelijk beschouwen kan in de toekomst ontdekt worden. Kijk bijvoorbeeld naar superintelligentie of teleportatie. Er zijn maar weinig mensen die hier in geloven dat dit ooit mogelijk zou kunnen zijn. Maar dat dacht men vroeger ook over videobellen en vliegende auto’s. Conclusie, alles wat je kan bedenken (theorieën) zou ooit daadwerkelijk ontwikkeld kunnen worden, zeker in de informatica.\n"
    }
  },
  "kader": {
    "default": "lijst van mogelijke opties is\n\n--maatschappelijk\n--wetenschappelijk\n\nTyp kader --<optie> voor meer informatie.",
    "flags": {
      "maatschappelijk": "\nNatural language processing wordt maatschappelijk veel gebruikt binnen vertalingen, stembediening en text-to-speech. Uit de huidige maatschappij zijn deze onderdelen niet weg te denken. Bijvoorbeeld als mensen op vakantie gaan, kunnen zij menu's lezen, met de inwoners communiceren of laten horen wat zij bedoelen aan de hand van Google Translate.  Ook is in recente jaren stembediening in populariteit gestegen. Voorbeelden daarvan zijn Alexa van Amazon, Siri van Apple en de Google assistent. Deze assistenten proberen een menselijke reactie te geven via stembediening. Door natural language processing kunnen vragen begrepen worden.\n",
      "wetenschappelijk": "\nNatural language processing kan wetenschappelijk gebruikt worden om texten te minen. Met de geminede data kunnen wetenschappers onderzoek verrichten. Alleen woorden filteren is dan niet genoeg omdat de betekenis van de zin van belang is. Bovendien kunnen papers vertaald worden of kunnen wetenschappers beter met elkaar communiceren. Door betere communicatie in de wetenschap kunnen samenwerkingen plaatsvinden die voorheen onmogelijk worden geacht. Ook zouden op ten duur wetenschappers in hun eigen taal een rapport kunnen schrijven, waardoor er misvattingen of menselijke taalfouten worden vermeden.\n"
    }
  },
  "toekomst": {
    "default":  "lijst van mogelijke opties is\n\n--taalbarriere\n--robots\n--computer\n\nTyp toekomst --<optie> voor meer informatie.",
    "flags": {
      "taalbarriere": "\nEr zijn verschillende toekomstbeelden mogelijk met de uitbreiding van NLP. Wat een grote impact kan hebben is machine translation. Stel je een wereld voor waarbij iedereen met elkaar kan communiceren, zonder taalbarrières. Dit zou namelijk betekenen dat alle mensen ter wereld met elkaar kunnen praten door middel van een apparaatje dat real-time talen kan herkennen en op een juiste manier kan vertalen. Dit betekent dat je nooit meer een nieuwe taal hoeft te leren (of zelfs 5 op VWO) en dat je nooit meer rare of verkeerde zinnen via Google Translate krijgt!\n",
      "robots": "\nWe hebben allemaal wel eens van het doem beeld gehoord dat robots de wereld gaan overnemen. Maar stel je nou eens voor dat we vrienden kunnen worden met robots? Dan moeten we elkaar natuurlijk wel eerst kunnen begrijpen. En daar speelt wederom NLP weer een grote rol in. Wanneer de mens en robots met elkaar kunnen communiceren zijn er talloze nieuwe mogelijkheden. Zo kan je robots inzetten om ouderen van betere zorg te voorzien en kunnen ze wellicht de eenzaamheid onder ouderen verhelpen. Met vriendelijke robots hoeft nooit iemand meer eenzaam te zijn.\n",
      "computer": "\nAls informatica studenten worden wij opgeleid tot echte informatici waarbij programmeren een grote rol speelt. Stel je eens voor dat je je computer slechts hoeft te vertellen wat voor programma het moet maken en je computer dit dan ook meteen kan doen? Er kunnen in de toekomst grote veranderingen plaatsvinden met betrekking tot de samenwerking tussen mens en computer. Als spraakherkenning optimaal werkt hoeven wij onze computers slechts te vertellen wat we willen. En je computer kan dan waarschijnlijk zelfs terug praten om vragen te stellen over wat het moet doen.\n"
    }
  },
  "invalid": {
    "default": "Dit commando is ongeldig of bevat ongeldige opties. Probeer opnieuw of Typ \"help\"."
  },
  "hello": {
    "default": "Welkom bij de AVI terminal!\nIn deze terminal kan je vragen stellen m.b.v commando's. Om een overzicht van alle commando's te krijgen, typ \"help\"\n"
  }
}
